# -*- coding: utf-8 -*-
"""Projet_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e953SRoXJZnjM80s5S9JSMEN0NikoI8c
"""

!pip install datasets transformers

print(dataset["train"][0])

# Notebook de pr√©traitement du dataset CoNLL-2003 pour BiLSTM-CRF

# Importation des biblioth√®ques n√©cessaires
import datasets
import numpy as np
import torch
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence

# Chargement du dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Affichage des premi√®res lignes pour comprendre la structure
print(dataset["train"][0])

# Chargement du tokenizer (on peut utiliser un mod√®le comme BERT pour tokenizer les tokens)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de pr√©traitement des donn√©es
def preprocess_data(examples):
    """
    Tokenisation et alignement des labels pour le dataset CoNLL-2003.
    """
    # V√©rifier que chaque entr√©e est bien une liste
    examples["tokens"] = [list(tokens) if isinstance(tokens, list) else [] for tokens in examples["tokens"]]
    examples["ner_tags"] = [list(tags) if isinstance(tags, list) else [] for tags in examples["ner_tags"]]

    # Tokenisation en batch
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    # Alignement des labels
    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Application du pr√©traitement
dataset = dataset.map(preprocess_data, batched=True)

# Conversion en tenseurs PyTorch
def convert_to_tensors(batch):
    return {k: torch.tensor(v) for k, v in batch.items() if k in ["input_ids", "attention_mask", "labels"]}

dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# V√©rification d'un exemple apr√®s pr√©traitement
print(dataset["train"][0])

from transformers import AutoTokenizer

# Charger un tokenizer pr√©-entra√Æn√© (BERT par d√©faut)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def preprocess_data(examples):
    tokenized_inputs = tokenizer(examples["tokens"],
                                 is_split_into_words=True,
                                 truncation=True,
                                 padding="max_length",
                                 max_length=128)

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx])  # Associer le label au premier sous-token
            else:
                label_ids.append(-100)  # Ignorer les autres sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    # Ajouter les labels aux donn√©es tokenis√©es
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le pr√©traitement
dataset = dataset.map(preprocess_data, batched=True)

print(dataset["train"][0])

from torch.utils.data import DataLoader

# Fonction pour transformer en tenseurs PyTorch
import torch
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    """G√®re le padding dynamique pour √©viter les erreurs de dimension."""

    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    attention_mask = [torch.tensor(item["attention_mask"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # Trouver la longueur maximale dans le batch
    max_len = max(len(x) for x in input_ids)

    # Appliquer le padding √† la longueur max
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = pad_sequence(labels, batch_first=True, padding_value=-100)

    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

# Cr√©ation du DataLoader
batch_size = 16
train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

from transformers import AutoTokenizer
from datasets import load_dataset

# Charger le dataset CoNLL-2003
dataset = load_dataset("conll2003")

# Charger le tokenizer (BERT par d√©faut)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de tokenization
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        padding="max_length",
        is_split_into_words=True,
        max_length=128
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word:
                label_ids.append(label[word_id])
            else:
                label_ids.append(-100)
            previous_word = word_id
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer la tokenization
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size, collate_fn=collate_fn)

# V√©rifie que le DataLoader fonctionne maintenant
for batch in train_dataloader:
    print(batch)
    break  # V√©rifie qu'un batch est bien r√©cup√©r√©

!pip install git+https://github.com/kmkurn/pytorch-crf.git

import torch
import torch.nn as nn
from torchcrf import CRF

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)

        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            loss = -self.crf(logits, labels.masked_fill(labels == -100, 0), mask=attention_mask.bool(), reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.byte())
            return predictions

# D√©finition des dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)
embedding_dim = 128
hidden_dim = 256

# instance du mod√®le
model = BiLSTM_CRF(vocab_size, tagset_size, embedding_dim, hidden_dim)

# Cr√©ation d'un batch factice
batch_size = 2
seq_length = 10  # Longueur des phrases

input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
attention_mask = torch.ones((batch_size, seq_length))
labels = torch.randint(0, tagset_size, (batch_size, seq_length))

# Passage avant forward
loss = model(input_ids, attention_mask, labels)
predictions = model(input_ids, attention_mask)

print(f"Loss: {loss.item()}")
print(f"Predictions: {predictions}")

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# Classe
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            #  Remplacement des `-100` uniquement pour le CRF
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            #  Application du masque pour √©viter les erreurs d'index
            mask = attention_mask.bool()

            # Calcul de la perte en √©vitant les labels inutiles
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions


# Fonction d'entra√Ænement
def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.001):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    writer = SummaryWriter()

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Ajout de la validation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

# D√©finition des dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)

#  Instancier le mod√®le
model = BiLSTM_CRF(vocab_size, tagset_size)

# Lancer l'entra√Ænement
train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.001)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs

!ls -lh runs

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs/Mar11_10-43-17_2a96257ce810

train_losses = [6.6452, 2.5228, 1.4345, 0.9181, 0.6116, 0.4412, 0.3304]
val_losses = [4.0255, 2.7370, 2.5212, 2.4313, 2.5663, 2.5773, 2.7883]

# Calculer l'√©cart relatif entre Train et Validation
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Afficher les r√©sultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: √âcart Train/Val = {gap:.4f}")

"""AJOUT DE R√©gularisation pour diminuer l'overfit"""

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.6):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer (dropout augment√© √† 0.6 pour r√©duire l'overfitting)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# Instancier le mod√®le
model = BiLSTM_CRF(vocab_size=30522, tagset_size=9)

# D√©finir l'optimiseur avec un learning rate plus bas et du weight decay
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)

train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

train_losses = [8.6759, 4.1344, 2.7293, 1.9545, 1.4366, 1.0997, 0.8396, 0.6583, 0.5132]
val_losses = [5.6604, 3.9413, 3.1119, 2.9799, 2.7796, 2.6574, 2.7641, 2.7759, 3.0640]

# Calcul de l‚Äô√©cart
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Affichage des r√©sultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: √âcart Train/Val = {gap:.4f}")

"""train_model() avec Scheduler"""

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0
            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# R√©instancier le mod√®le avec les bonnes dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)

model = BiLSTM_CRF(vocab_size, tagset_size)

def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter()

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Validation loop
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        # Ajout du Scheduler
        scheduler.step(avg_val_loss)  # R√©duit le learning rate si la val loss stagne

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

print(train_dataloader if 'train_dataloader' in globals() else "train_dataloader non d√©fini")
print(val_dataloader if 'val_dataloader' in globals() else "val_dataloader non d√©fini")

from datasets import load_dataset

# Charger le dataset CoNLL-2003
dataset = load_dataset("conll2003")

# V√©rifier que les donn√©es sont bien charg√©es
print(dataset)

from transformers import AutoTokenizer
from torch.utils.data import DataLoader

# Utiliser un tokenizer (ex: BERT)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", is_split_into_words=True)

    # Aligner les labels avec les tokens
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word:
                label_ids.append(label[word_id])
            else:
                label_ids.append(-100)
            previous_word = word_id
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer la tokenization
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

# Transformer en DataLoader PyTorch
batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size)

from torch.utils.tensorboard import SummaryWriter

print("Mod√®le :", "OK" if 'model' in globals() else "Non d√©fini")
print("Train Dataloader :", "OK" if 'train_dataloader' in globals() else "Non d√©fini")
print("Val Dataloader :", "OK" if 'val_dataloader' in globals() else "Non d√©fini")
print("SummaryWriter :", "OK" if 'SummaryWriter' in globals() else "Non d√©fini")

from torch.utils.data import DataLoader
import torch

def collate_fn(batch):
    """Fonction de padding dynamique pour √©viter les erreurs de tailles de batch."""
    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    attention_mask = [torch.tensor(item["attention_mask"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # Trouver la longueur maximale dans le batch
    max_len = max(len(x) for x in input_ids)

    # Appliquer le padding √† la longueur max
    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size, collate_fn=collate_fn)

train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005)

!pip show tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

train_losses = [8.3645, 3.8663, 2.4594, 1.7075, 1.2257, 0.9073, 0.6801, 0.5249, 0.4124, 0.3306]
val_losses = [5.4177, 3.6590, 3.1378, 2.6252, 2.5953, 2.3183, 2.4702, 2.2578, 2.3229, 2.7760]

# Calcul de l‚Äô√©cart
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Affichage des r√©sultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: √âcart Train/Val = {gap:.4f}")

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            loss = -self.crf(logits, labels, mask=attention_mask.bool(), reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter()  # Cr√©ation d'un dossier runs/ pour TensorBoard

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            #  Ajout du gradient des poids pour suivre l'√©volution
            for name, param in model.named_parameters():
                writer.add_histogram(f"Gradients/{name}", param.grad, epoch)

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        #  Ajouter l'histogramme des poids
        for name, param in model.named_parameters():
            writer.add_histogram(f"Weights/{name}", param, epoch)

        # Validation loop
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        #  Ajout du Learning Rate sur TensorBoard
        scheduler.step(avg_val_loss)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

"""Recherche d'hyperparam√®tres:

V√©rifier la perte initiale du mod√®le
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
import numpy as np

#  D√©finition du mod√®le BiLSTM-CRF
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_labels)

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        packed_output, _ = self.lstm(embeddings)
        outputs = self.fc(packed_output)
        return outputs  # (batch_size, seq_len, num_labels)

#  Initialisation du mod√®le
vocab_size = 10000
embedding_dim = 128
hidden_dim = 256
num_labels = 9

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels).to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

#  Pr√©paration des donn√©es (donn√©es fictives pour test)
batch_size = 16
seq_length = 10

inputs = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)
targets = torch.randint(0, num_labels, (batch_size, seq_length)).to(device)
attention_mask = torch.ones_like(inputs).to(device)  # 1 = token valide, 0 = padding

dataset = TensorDataset(inputs, targets, attention_mask)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

#  Initialisation de TensorBoard
writer = SummaryWriter("runs/BiLSTM_CRF")

#  V√©rification de la perte initiale
with torch.no_grad():
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    writer.add_scalar("Loss/Initial", loss.item(), 0)
    print(f"Perte initiale : {loss.item():.4f}")

#  Overfit intentionnellement sur un petit √©chantillon
print("\n Overfitting sur un petit √©chantillon...")
for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    optimizer.step()

    writer.add_scalar("Loss/Overfit", loss.item(), epoch)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

#  Ajout de Dropout dans le BiLSTM
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=0.3)  # üî• Dropout ajout√©
        self.fc = nn.Linear(hidden_dim * 2, num_labels)

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        packed_output, _ = self.lstm(embeddings)
        outputs = self.fc(packed_output)
        return outputs  # (batch_size, seq_len, num_labels)

# --- Test sur un √©chantillon non vu apr√®s l‚Äôoverfit ---
print("\n V√©rification sur un √©chantillon hors entra√Ænement...")

# Nouveau batch al√©atoire pour test
test_inputs = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)
test_targets = torch.randint(0, num_labels, (batch_size, seq_length)).to(device)
test_attention_mask = torch.ones_like(test_inputs).to(device)

with torch.no_grad():
    test_outputs = model(test_inputs, test_attention_mask)
    test_loss = loss_function(test_outputs.view(-1, num_labels), test_targets.view(-1))
    print(f" Perte sur un √©chantillon non vu : {test_loss.item():.4f}")  # Attendu : perte √©lev√©e si overfitting

#  Trouver un bon learning rate
print("\n Recherche d'un bon learning rate...")
learning_rates = [0.1, 0.01, 0.001, 0.0001]
losses = []
for lr in learning_rates:
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    optimizer.step()

    losses.append(loss.item())
    writer.add_scalar(f"Loss/LR_{lr}", loss.item(), 0)

#  Recherche en grille des hyperparam√®tres
print("\n Grid Search sur les hyperparam√®tres...")
grid_hidden_dims = [128, 256, 512]
best_model = None
best_loss = float("inf")

for hidden_dim in grid_hidden_dims:
    temp_model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels).to(device)
    temp_optimizer = optim.Adam(temp_model.parameters(), lr=0.01)

    temp_model.train()
    temp_optimizer.zero_grad()
    outputs = temp_model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    temp_optimizer.step()

    writer.add_scalar(f"Loss/HiddenDim_{hidden_dim}", loss.item(), 0)

    if loss.item() < best_loss:
        best_loss = loss.item()
        best_model = temp_model

"""Ajouter plus de couches LSTM"""

import torch
import torch.nn as nn
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, dropout=0.3, num_layers=2):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, num_layers=num_layers,
            batch_first=True, bidirectional=True, dropout=dropout
        )
        self.dropout = nn.Dropout(dropout)  # Dropout apr√®s le LSTM
        self.fc = nn.Linear(hidden_dim * 2, num_labels)  # x2 car bidirectionnel

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        lstm_out, _ = self.lstm(embeddings)
        dropped_out = self.dropout(lstm_out)  #Application du dropout
        outputs = self.fc(dropped_out)
        return outputs

# Dummy parameters pour tester
vocab_size = 1000
embedding_dim = 100
hidden_dim = 256
num_labels = 5
sequence_length = 10
batch_size = 2

# Cr√©ation du mod√®le
model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels)

# Test avec des inputs al√©atoires
inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
attention_mask = torch.ones(batch_size, sequence_length)  # Masque fictif
outputs = model(inputs, attention_mask)

print(" Le mod√®le tourne sans erreur Dimensions de sortie :", outputs.shape)

import itertools

# D√©finition des valeurs √† tester
param_grid = {
    "learning_rate": [1e-3, 5e-4, 1e-4],
    "hidden_dim": [128, 256, 512],
    "dropout": [0.3, 0.5]
}

# Liste toutes les combinaisons possibles
param_combinations = list(itertools.product(*param_grid.values()))

best_loss = float("inf")
best_params = None

for params in param_combinations:
    lr, hidden_dim, dropout = params
    print(f"\nTest avec lr={lr}, hidden_dim={hidden_dim}, dropout={dropout}")

    # Initialisation du mod√®le avec ces param√®tres
    model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels, dropout=dropout, num_layers=2)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_function = nn.CrossEntropyLoss()

    # Entra√Ænement rapide (quelques epochs)
    for epoch in range(5):
        model.train()
        optimizer.zero_grad()
        outputs = model(inputs, attention_mask)
        loss = loss_function(outputs.view(-1, num_labels), torch.randint(0, num_labels, (batch_size * sequence_length,)))
        loss.backward()
        optimizer.step()

    # V√©rification de la perte
    if loss.item() < best_loss:
        best_loss = loss.item()
        best_params = params

print(f"\n Meilleurs param√®tres trouv√©s : lr={best_params[0]}, hidden_dim={best_params[1]}, dropout={best_params[2]}")

"""\\meilleur param√®tres : lr=0.001, hidden_dim=256, dropout=0.5"""

# Initialisation de TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("runs/final_training")

# Chargement du mod√®le avec les meilleurs param√®tres
best_lr = 0.001
best_hidden_dim = 256
best_dropout = 0.5

model = BiLSTM_CRF(vocab_size, embedding_dim, best_hidden_dim, num_labels, dropout=best_dropout, num_layers=2)
optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)

# Entra√Ænement principal
num_epochs = 50  # Augmenter si besoin
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), torch.randint(0, num_labels, (batch_size * sequence_length,)))
    loss.backward()
    optimizer.step()

    # Enregistrer la loss dans TensorBoard
    writer.add_scalar('Loss/Training', loss.item(), epoch)

    # Affichage toutes les 10 epochs
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Training Loss: {loss.item():.4f}")

writer.close()

tensorboard --logdir=runs

"""√âvaluer le mod√®le sur des donn√©es non vues"""

# S√©lection d'un petit √©chantillon de validation
validation_size = 16  # Ajuste la taille selon ton dataset

# G√©n√©ration al√©atoire d'un √©chantillon de validation (√† remplacer par de vraies donn√©es)
validation_inputs = torch.randint(0, vocab_size, (validation_size, sequence_length))
validation_attention_mask = torch.ones_like(validation_inputs)  # Masque d'attention (1 partout)
validation_labels = torch.randint(0, num_labels, (validation_size, sequence_length))  # √âtiquettes al√©atoires

print(f" Dimensions des donn√©es de validation : {validation_inputs.shape}")
print(f"Dimensions du masque d‚Äôattention : {validation_attention_mask.shape}")
print(f"Dimensions des labels : {validation_labels.shape}")

model.eval()
with torch.no_grad():
    outputs = model(validation_inputs, validation_attention_mask)
    loss_val = loss_function(outputs.view(-1, num_labels), validation_labels.view(-1))

print(f" Perte sur un √©chantillon de validation : {loss_val.item():.4f}")

import torch
from torch.utils.tensorboard import SummaryWriter
import os

# D√©finir le r√©pertoire o√π stocker les logs TensorBoard
log_dir = "./logs"
os.makedirs(log_dir, exist_ok=True)

# Initialiser TensorBoard
writer = SummaryWriter(log_dir=log_dir)

from torch.utils.data import DataLoader, TensorDataset

# Simule un dataset avec des tensors al√©atoires
batch_size = 16
sequence_length = 10
num_labels = 5  # Nombre de classes pour la classification

# G√©n√©ration de donn√©es fictives
train_inputs = torch.randint(0, 100, (100, sequence_length))  # 100 phrases avec 10 tokens chacune
train_attention_masks = torch.ones(100, sequence_length)  # Masques d'attention
train_labels = torch.randint(0, num_labels, (100, sequence_length))  # Labels pour chaque token

val_inputs = torch.randint(0, 100, (20, sequence_length))
val_attention_masks = torch.ones(20, sequence_length)
val_labels = torch.randint(0, num_labels, (20, sequence_length))

# Cr√©ation des datasets PyTorch
train_dataset = TensorDataset(train_inputs, train_attention_masks, train_labels)
val_dataset = TensorDataset(val_inputs, val_attention_masks, val_labels)

# Cr√©ation des DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# V√©rification
print(f"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}")
print(f"Exemple batch: {next(iter(train_loader))}")

num_epochs = 10  # Commence avec peu d'√©poques pour tester

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for inputs, attention_masks, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs, attention_masks)

        loss = loss_function(outputs.view(-1, num_labels), labels.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        preds = torch.argmax(outputs, dim=-1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.numel()

    train_loss = running_loss / len(train_loader)
    train_acc = correct_predictions / total_predictions

    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

writer.close()

from torch.utils.tensorboard import SummaryWriter

# Cr√©er un dossier pour TensorBoard logs
writer = SummaryWriter("runs/BiLSTM_CRF")

num_epochs = 20  # Augmenter le nombre d'√©poques pour mieux observer l'√©volution

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for inputs, attention_masks, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs, attention_masks)

        loss = loss_function(outputs.view(-1, num_labels), labels.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        preds = torch.argmax(outputs, dim=-1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.numel()

    train_loss = running_loss / len(train_loader)
    train_acc = correct_predictions / total_predictions

    # Enregistrer les valeurs dans TensorBoard
    writer.add_scalar("Loss/Train", train_loss, epoch)
    writer.add_scalar("Accuracy/Train", train_acc, epoch)

    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs

!kill 23696

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

"""Observation vanishing/ exploding"""

!pip install datasets transformers torch torchcrf tensorboard

import datasets
import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter

# Charger le dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Charger le tokenizer BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de pr√©traitement des donn√©es
def preprocess_data(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le pr√©traitement
dataset = dataset.map(preprocess_data, batched=True)

# Convertir en format PyTorch
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

def collate_fn(batch):
    """
    Fonction pour traiter les batchs et √©viter les warnings.
    """
    return {
        "input_ids": torch.stack([item["input_ids"].clone().detach() for item in batch]),
        "attention_mask": torch.stack([item["attention_mask"].clone().detach() for item in batch]),
        "labels": torch.stack([item["labels"].clone().detach() for item in batch])
    }

batch_size = 16

train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

def train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter("runs/BiLSTM_CRF_gradients")

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()

            #  Log des gradients et des poids pour analyser le vanishing/exploding gradient
            for name, param in model.named_parameters():
                if param.grad is not None:
                    writer.add_histogram(f"Gradients/{name}", param.grad, epoch)
                writer.add_histogram(f"Weights/{name}", param, epoch)

            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Planification du Learning Rate
        scheduler.step(avg_train_loss)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_train_loss < best_val_loss:
            best_val_loss = avg_train_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

vocab_size = 30522  # Pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER

# Instancier le mod√®le
model = BiLSTM_CRF(vocab_size, tagset_size)

# Lancer l'entra√Ænement
train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005)

import os
log_dir = "runs/BiLSTM_CRF_gradients"
print("Fichiers dans runs/BiLSTM_CRF_gradients:", os.listdir(log_dir) if os.path.exists(log_dir) else "Pas de logs trouv√©s")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs/BiLSTM_CRF_gradients

!pip install torch torchvision torchaudio transformers datasets torchcrf tensorboard

!pip install git+https://github.com/kmkurn/pytorch-crf.git

import datasets
import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter

# Charger le dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Charger le tokenizer BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de pr√©traitement des donn√©es
def preprocess_data(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le pr√©traitement
dataset = dataset.map(preprocess_data, batched=True)

# Convertir en format PyTorch
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

def collate_fn(batch):
    """
    Fonction pour traiter les batchs et √©viter les warnings.
    """
    return {
        "input_ids": torch.stack([item["input_ids"].clone().detach() for item in batch]),
        "attention_mask": torch.stack([item["attention_mask"].clone().detach() for item in batch]),
        "labels": torch.stack([item["labels"].clone().detach() for item in batch])
    }

batch_size = 16

train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader

# D√©finition du mod√®le BiLSTM-CRF
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Fully Connected Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# Fonction d'entra√Ænement avec logs TensorBoard
def train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter("runs/BiLSTM_CRF_retrain")
    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()

            optimizer.step()
            total_loss += loss.item()

            # Calcul de l'accuracy
            preds = model(input_ids, attention_mask)
            for i in range(len(preds)):
                correct_predictions += sum([1 for p, l in zip(preds[i], labels[i].tolist()) if l != -100 and p == l])
                total_predictions += sum([1 for l in labels[i].tolist() if l != -100])

        avg_train_loss = total_loss / len(train_dataloader)
        train_accuracy = correct_predictions / total_predictions

        # Enregistrer les m√©triques dans TensorBoard
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)
        writer.add_scalar("Accuracy/Train", train_accuracy, epoch)

        # Phase de validation
        model.eval()
        total_val_loss = 0
        correct_val_predictions = 0
        total_val_predictions = 0

        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

                preds = model(input_ids, attention_mask)
                for i in range(len(preds)):
                    correct_val_predictions += sum([1 for p, l in zip(preds[i], labels[i].tolist()) if l != -100 and p == l])
                    total_val_predictions += sum([1 for l in labels[i].tolist() if l != -100])

        avg_val_loss = total_val_loss / len(val_dataloader)
        val_accuracy = correct_val_predictions / total_val_predictions

        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)
        writer.add_scalar("Accuracy/Validation", val_accuracy, epoch)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        # Ajustement du learning rate avec le scheduler
        scheduler.step(avg_val_loss)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_accuracy:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_accuracy:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

# R√©initialiser et entra√Æner √† nouveau
vocab_size = 30522  # Tokenizer BERT
tagset_size = 9  # Classes NER

model = BiLSTM_CRF(vocab_size, tagset_size)

train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005)

# Commented out IPython magic to ensure Python compatibility.
# Charger l'extension TensorBoard
# %load_ext tensorboard

# Lancer TensorBoard
# %tensorboard --logdir=runs/BiLSTM_CRF_retrain

import os

log_dir = "runs/BiLSTM_CRF_retrain"
print("Fichiers dans runs/BiLSTM_CRF_retrain:", os.listdir(log_dir))