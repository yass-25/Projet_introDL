# -*- coding: utf-8 -*-
"""Projet_DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e953SRoXJZnjM80s5S9JSMEN0NikoI8c
"""

!pip install datasets transformers

print(dataset["train"][0])

# Notebook de prétraitement du dataset CoNLL-2003 pour BiLSTM-CRF

# Importation des bibliothèques nécessaires
import datasets
import numpy as np
import torch
from transformers import AutoTokenizer
from torch.nn.utils.rnn import pad_sequence

# Chargement du dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Affichage des premières lignes pour comprendre la structure
print(dataset["train"][0])

# Chargement du tokenizer (on peut utiliser un modèle comme BERT pour tokenizer les tokens)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de prétraitement des données
def preprocess_data(examples):
    """
    Tokenisation et alignement des labels pour le dataset CoNLL-2003.
    """
    # Vérifier que chaque entrée est bien une liste
    examples["tokens"] = [list(tokens) if isinstance(tokens, list) else [] for tokens in examples["tokens"]]
    examples["ner_tags"] = [list(tags) if isinstance(tags, list) else [] for tags in examples["ner_tags"]]

    # Tokenisation en batch
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    # Alignement des labels
    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Application du prétraitement
dataset = dataset.map(preprocess_data, batched=True)

# Conversion en tenseurs PyTorch
def convert_to_tensors(batch):
    return {k: torch.tensor(v) for k, v in batch.items() if k in ["input_ids", "attention_mask", "labels"]}

dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

# Vérification d'un exemple après prétraitement
print(dataset["train"][0])

from transformers import AutoTokenizer

# Charger un tokenizer pré-entraîné (BERT par défaut)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def preprocess_data(examples):
    tokenized_inputs = tokenizer(examples["tokens"],
                                 is_split_into_words=True,
                                 truncation=True,
                                 padding="max_length",
                                 max_length=128)

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignore padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx])  # Associer le label au premier sous-token
            else:
                label_ids.append(-100)  # Ignorer les autres sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    # Ajouter les labels aux données tokenisées
    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le prétraitement
dataset = dataset.map(preprocess_data, batched=True)

print(dataset["train"][0])

from torch.utils.data import DataLoader

# Fonction pour transformer en tenseurs PyTorch
import torch
from torch.nn.utils.rnn import pad_sequence

def collate_fn(batch):
    """Gère le padding dynamique pour éviter les erreurs de dimension."""

    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    attention_mask = [torch.tensor(item["attention_mask"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # Trouver la longueur maximale dans le batch
    max_len = max(len(x) for x in input_ids)

    # Appliquer le padding à la longueur max
    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = pad_sequence(labels, batch_first=True, padding_value=-100)

    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

# Création du DataLoader
batch_size = 16
train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

from transformers import AutoTokenizer
from datasets import load_dataset

# Charger le dataset CoNLL-2003
dataset = load_dataset("conll2003")

# Charger le tokenizer (BERT par défaut)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de tokenization
def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        truncation=True,
        padding="max_length",
        is_split_into_words=True,
        max_length=128
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word:
                label_ids.append(label[word_id])
            else:
                label_ids.append(-100)
            previous_word = word_id
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer la tokenization
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size, collate_fn=collate_fn)

# Vérifie que le DataLoader fonctionne maintenant
for batch in train_dataloader:
    print(batch)
    break  # Vérifie qu'un batch est bien récupéré

!pip install git+https://github.com/kmkurn/pytorch-crf.git

import torch
import torch.nn as nn
from torchcrf import CRF

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)

        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            loss = -self.crf(logits, labels.masked_fill(labels == -100, 0), mask=attention_mask.bool(), reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.byte())
            return predictions

# Définition des dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)
embedding_dim = 128
hidden_dim = 256

# instance du modèle
model = BiLSTM_CRF(vocab_size, tagset_size, embedding_dim, hidden_dim)

# Création d'un batch factice
batch_size = 2
seq_length = 10  # Longueur des phrases

input_ids = torch.randint(0, vocab_size, (batch_size, seq_length))
attention_mask = torch.ones((batch_size, seq_length))
labels = torch.randint(0, tagset_size, (batch_size, seq_length))

# Passage avant forward
loss = model(input_ids, attention_mask, labels)
predictions = model(input_ids, attention_mask)

print(f"Loss: {loss.item()}")
print(f"Predictions: {predictions}")

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

# Classe
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            #  Remplacement des `-100` uniquement pour le CRF
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            #  Application du masque pour éviter les erreurs d'index
            mask = attention_mask.bool()

            # Calcul de la perte en évitant les labels inutiles
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions


# Fonction d'entraînement
def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.001):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    writer = SummaryWriter()

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Ajout de la validation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

# Définition des dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)

#  Instancier le modèle
model = BiLSTM_CRF(vocab_size, tagset_size)

# Lancer l'entraînement
train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.001)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs

!ls -lh runs

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs/Mar11_10-43-17_2a96257ce810

train_losses = [6.6452, 2.5228, 1.4345, 0.9181, 0.6116, 0.4412, 0.3304]
val_losses = [4.0255, 2.7370, 2.5212, 2.4313, 2.5663, 2.5773, 2.7883]

# Calculer l'écart relatif entre Train et Validation
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Afficher les résultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: Écart Train/Val = {gap:.4f}")

"""AJOUT DE Régularisation pour diminuer l'overfit"""

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.6):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer (dropout augmenté à 0.6 pour réduire l'overfitting)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# Instancier le modèle
model = BiLSTM_CRF(vocab_size=30522, tagset_size=9)

# Définir l'optimiseur avec un learning rate plus bas et du weight decay
optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-4)

train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005)

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

train_losses = [8.6759, 4.1344, 2.7293, 1.9545, 1.4366, 1.0997, 0.8396, 0.6583, 0.5132]
val_losses = [5.6604, 3.9413, 3.1119, 2.9799, 2.7796, 2.6574, 2.7641, 2.7759, 3.0640]

# Calcul de l’écart
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Affichage des résultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: Écart Train/Val = {gap:.4f}")

"""train_model() avec Scheduler"""

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0
            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# Réinstancier le modèle avec les bonnes dimensions
vocab_size = 30522  # Exemple pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER (ex: CoNLL-2003)

model = BiLSTM_CRF(vocab_size, tagset_size)

def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter()

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Validation loop
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        # Ajout du Scheduler
        scheduler.step(avg_val_loss)  # Réduit le learning rate si la val loss stagne

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

print(train_dataloader if 'train_dataloader' in globals() else "train_dataloader non défini")
print(val_dataloader if 'val_dataloader' in globals() else "val_dataloader non défini")

from datasets import load_dataset

# Charger le dataset CoNLL-2003
dataset = load_dataset("conll2003")

# Vérifier que les données sont bien chargées
print(dataset)

from transformers import AutoTokenizer
from torch.utils.data import DataLoader

# Utiliser un tokenizer (ex: BERT)
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, padding="max_length", is_split_into_words=True)

    # Aligner les labels avec les tokens
    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word = None
        label_ids = []
        for word_id in word_ids:
            if word_id is None:
                label_ids.append(-100)
            elif word_id != previous_word:
                label_ids.append(label[word_id])
            else:
                label_ids.append(-100)
            previous_word = word_id
        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer la tokenization
tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)

# Transformer en DataLoader PyTorch
batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size)

from torch.utils.tensorboard import SummaryWriter

print("Modèle :", "OK" if 'model' in globals() else "Non défini")
print("Train Dataloader :", "OK" if 'train_dataloader' in globals() else "Non défini")
print("Val Dataloader :", "OK" if 'val_dataloader' in globals() else "Non défini")
print("SummaryWriter :", "OK" if 'SummaryWriter' in globals() else "Non défini")

from torch.utils.data import DataLoader
import torch

def collate_fn(batch):
    """Fonction de padding dynamique pour éviter les erreurs de tailles de batch."""
    input_ids = [torch.tensor(item["input_ids"]) for item in batch]
    attention_mask = [torch.tensor(item["attention_mask"]) for item in batch]
    labels = [torch.tensor(item["labels"]) for item in batch]

    # Trouver la longueur maximale dans le batch
    max_len = max(len(x) for x in input_ids)

    # Appliquer le padding à la longueur max
    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=0)
    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)

    return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

batch_size = 16

train_dataloader = DataLoader(tokenized_datasets["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(tokenized_datasets["validation"], batch_size=batch_size, collate_fn=collate_fn)

train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005)

!pip show tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

train_losses = [8.3645, 3.8663, 2.4594, 1.7075, 1.2257, 0.9073, 0.6801, 0.5249, 0.4124, 0.3306]
val_losses = [5.4177, 3.6590, 3.1378, 2.6252, 2.5953, 2.3183, 2.4702, 2.2578, 2.3229, 2.7760]

# Calcul de l’écart
loss_gaps = [val - train for train, val in zip(train_losses, val_losses)]

# Affichage des résultats
for epoch, gap in enumerate(loss_gaps, start=1):
    print(f"Epoch {epoch}: Écart Train/Val = {gap:.4f}")

import torch
import torch.nn as nn
from torchcrf import CRF
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Linear Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            loss = -self.crf(logits, labels, mask=attention_mask.bool(), reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

def train_model(model, train_dataloader, val_dataloader, epochs=10, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter()  # Création d'un dossier runs/ pour TensorBoard

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

            #  Ajout du gradient des poids pour suivre l'évolution
            for name, param in model.named_parameters():
                writer.add_histogram(f"Gradients/{name}", param.grad, epoch)

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        #  Ajouter l'histogramme des poids
        for name, param in model.named_parameters():
            writer.add_histogram(f"Weights/{name}", param, epoch)

        # Validation loop
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_dataloader)
        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)

        #  Ajout du Learning Rate sur TensorBoard
        scheduler.step(avg_val_loss)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

"""Recherche d'hyperparamètres:

Vérifier la perte initiale du modèle
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
import numpy as np

#  Définition du modèle BiLSTM-CRF
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden_dim * 2, num_labels)

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        packed_output, _ = self.lstm(embeddings)
        outputs = self.fc(packed_output)
        return outputs  # (batch_size, seq_len, num_labels)

#  Initialisation du modèle
vocab_size = 10000
embedding_dim = 128
hidden_dim = 256
num_labels = 9

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels).to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

#  Préparation des données (données fictives pour test)
batch_size = 16
seq_length = 10

inputs = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)
targets = torch.randint(0, num_labels, (batch_size, seq_length)).to(device)
attention_mask = torch.ones_like(inputs).to(device)  # 1 = token valide, 0 = padding

dataset = TensorDataset(inputs, targets, attention_mask)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

#  Initialisation de TensorBoard
writer = SummaryWriter("runs/BiLSTM_CRF")

#  Vérification de la perte initiale
with torch.no_grad():
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    writer.add_scalar("Loss/Initial", loss.item(), 0)
    print(f"Perte initiale : {loss.item():.4f}")

#  Overfit intentionnellement sur un petit échantillon
print("\n Overfitting sur un petit échantillon...")
for epoch in range(100):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    optimizer.step()

    writer.add_scalar("Loss/Overfit", loss.item(), epoch)

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

#  Ajout de Dropout dans le BiLSTM
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, dropout=0.3)  # 🔥 Dropout ajouté
        self.fc = nn.Linear(hidden_dim * 2, num_labels)

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        packed_output, _ = self.lstm(embeddings)
        outputs = self.fc(packed_output)
        return outputs  # (batch_size, seq_len, num_labels)

# --- Test sur un échantillon non vu après l’overfit ---
print("\n Vérification sur un échantillon hors entraînement...")

# Nouveau batch aléatoire pour test
test_inputs = torch.randint(0, vocab_size, (batch_size, seq_length)).to(device)
test_targets = torch.randint(0, num_labels, (batch_size, seq_length)).to(device)
test_attention_mask = torch.ones_like(test_inputs).to(device)

with torch.no_grad():
    test_outputs = model(test_inputs, test_attention_mask)
    test_loss = loss_function(test_outputs.view(-1, num_labels), test_targets.view(-1))
    print(f" Perte sur un échantillon non vu : {test_loss.item():.4f}")  # Attendu : perte élevée si overfitting

#  Trouver un bon learning rate
print("\n Recherche d'un bon learning rate...")
learning_rates = [0.1, 0.01, 0.001, 0.0001]
losses = []
for lr in learning_rates:
    optimizer = optim.Adam(model.parameters(), lr=lr)
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    optimizer.step()

    losses.append(loss.item())
    writer.add_scalar(f"Loss/LR_{lr}", loss.item(), 0)

#  Recherche en grille des hyperparamètres
print("\n Grid Search sur les hyperparamètres...")
grid_hidden_dims = [128, 256, 512]
best_model = None
best_loss = float("inf")

for hidden_dim in grid_hidden_dims:
    temp_model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels).to(device)
    temp_optimizer = optim.Adam(temp_model.parameters(), lr=0.01)

    temp_model.train()
    temp_optimizer.zero_grad()
    outputs = temp_model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), targets.view(-1))
    loss.backward()
    temp_optimizer.step()

    writer.add_scalar(f"Loss/HiddenDim_{hidden_dim}", loss.item(), 0)

    if loss.item() < best_loss:
        best_loss = loss.item()
        best_model = temp_model

"""Ajouter plus de couches LSTM"""

import torch
import torch.nn as nn
import torch.optim as optim

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_labels, dropout=0.3, num_layers=2):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            embedding_dim, hidden_dim, num_layers=num_layers,
            batch_first=True, bidirectional=True, dropout=dropout
        )
        self.dropout = nn.Dropout(dropout)  # Dropout après le LSTM
        self.fc = nn.Linear(hidden_dim * 2, num_labels)  # x2 car bidirectionnel

    def forward(self, x, attention_mask):
        embeddings = self.embedding(x)
        lstm_out, _ = self.lstm(embeddings)
        dropped_out = self.dropout(lstm_out)  #Application du dropout
        outputs = self.fc(dropped_out)
        return outputs

# Dummy parameters pour tester
vocab_size = 1000
embedding_dim = 100
hidden_dim = 256
num_labels = 5
sequence_length = 10
batch_size = 2

# Création du modèle
model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels)

# Test avec des inputs aléatoires
inputs = torch.randint(0, vocab_size, (batch_size, sequence_length))
attention_mask = torch.ones(batch_size, sequence_length)  # Masque fictif
outputs = model(inputs, attention_mask)

print(" Le modèle tourne sans erreur Dimensions de sortie :", outputs.shape)

import itertools

# Définition des valeurs à tester
param_grid = {
    "learning_rate": [1e-3, 5e-4, 1e-4],
    "hidden_dim": [128, 256, 512],
    "dropout": [0.3, 0.5]
}

# Liste toutes les combinaisons possibles
param_combinations = list(itertools.product(*param_grid.values()))

best_loss = float("inf")
best_params = None

for params in param_combinations:
    lr, hidden_dim, dropout = params
    print(f"\nTest avec lr={lr}, hidden_dim={hidden_dim}, dropout={dropout}")

    # Initialisation du modèle avec ces paramètres
    model = BiLSTM_CRF(vocab_size, embedding_dim, hidden_dim, num_labels, dropout=dropout, num_layers=2)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_function = nn.CrossEntropyLoss()

    # Entraînement rapide (quelques epochs)
    for epoch in range(5):
        model.train()
        optimizer.zero_grad()
        outputs = model(inputs, attention_mask)
        loss = loss_function(outputs.view(-1, num_labels), torch.randint(0, num_labels, (batch_size * sequence_length,)))
        loss.backward()
        optimizer.step()

    # Vérification de la perte
    if loss.item() < best_loss:
        best_loss = loss.item()
        best_params = params

print(f"\n Meilleurs paramètres trouvés : lr={best_params[0]}, hidden_dim={best_params[1]}, dropout={best_params[2]}")

"""\\meilleur paramètres : lr=0.001, hidden_dim=256, dropout=0.5"""

# Initialisation de TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("runs/final_training")

# Chargement du modèle avec les meilleurs paramètres
best_lr = 0.001
best_hidden_dim = 256
best_dropout = 0.5

model = BiLSTM_CRF(vocab_size, embedding_dim, best_hidden_dim, num_labels, dropout=best_dropout, num_layers=2)
optimizer = torch.optim.Adam(model.parameters(), lr=best_lr)

# Entraînement principal
num_epochs = 50  # Augmenter si besoin
for epoch in range(num_epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs, attention_mask)
    loss = loss_function(outputs.view(-1, num_labels), torch.randint(0, num_labels, (batch_size * sequence_length,)))
    loss.backward()
    optimizer.step()

    # Enregistrer la loss dans TensorBoard
    writer.add_scalar('Loss/Training', loss.item(), epoch)

    # Affichage toutes les 10 epochs
    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Training Loss: {loss.item():.4f}")

writer.close()

tensorboard --logdir=runs

"""Évaluer le modèle sur des données non vues"""

# Sélection d'un petit échantillon de validation
validation_size = 16  # Ajuste la taille selon ton dataset

# Génération aléatoire d'un échantillon de validation (à remplacer par de vraies données)
validation_inputs = torch.randint(0, vocab_size, (validation_size, sequence_length))
validation_attention_mask = torch.ones_like(validation_inputs)  # Masque d'attention (1 partout)
validation_labels = torch.randint(0, num_labels, (validation_size, sequence_length))  # Étiquettes aléatoires

print(f" Dimensions des données de validation : {validation_inputs.shape}")
print(f"Dimensions du masque d’attention : {validation_attention_mask.shape}")
print(f"Dimensions des labels : {validation_labels.shape}")

model.eval()
with torch.no_grad():
    outputs = model(validation_inputs, validation_attention_mask)
    loss_val = loss_function(outputs.view(-1, num_labels), validation_labels.view(-1))

print(f" Perte sur un échantillon de validation : {loss_val.item():.4f}")

import torch
from torch.utils.tensorboard import SummaryWriter
import os

# Définir le répertoire où stocker les logs TensorBoard
log_dir = "./logs"
os.makedirs(log_dir, exist_ok=True)

# Initialiser TensorBoard
writer = SummaryWriter(log_dir=log_dir)

from torch.utils.data import DataLoader, TensorDataset

# Simule un dataset avec des tensors aléatoires
batch_size = 16
sequence_length = 10
num_labels = 5  # Nombre de classes pour la classification

# Génération de données fictives
train_inputs = torch.randint(0, 100, (100, sequence_length))  # 100 phrases avec 10 tokens chacune
train_attention_masks = torch.ones(100, sequence_length)  # Masques d'attention
train_labels = torch.randint(0, num_labels, (100, sequence_length))  # Labels pour chaque token

val_inputs = torch.randint(0, 100, (20, sequence_length))
val_attention_masks = torch.ones(20, sequence_length)
val_labels = torch.randint(0, num_labels, (20, sequence_length))

# Création des datasets PyTorch
train_dataset = TensorDataset(train_inputs, train_attention_masks, train_labels)
val_dataset = TensorDataset(val_inputs, val_attention_masks, val_labels)

# Création des DataLoaders
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Vérification
print(f"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}")
print(f"Exemple batch: {next(iter(train_loader))}")

num_epochs = 10  # Commence avec peu d'époques pour tester

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for inputs, attention_masks, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs, attention_masks)

        loss = loss_function(outputs.view(-1, num_labels), labels.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        preds = torch.argmax(outputs, dim=-1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.numel()

    train_loss = running_loss / len(train_loader)
    train_acc = correct_predictions / total_predictions

    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

writer.close()

from torch.utils.tensorboard import SummaryWriter

# Créer un dossier pour TensorBoard logs
writer = SummaryWriter("runs/BiLSTM_CRF")

num_epochs = 20  # Augmenter le nombre d'époques pour mieux observer l'évolution

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct_predictions = 0
    total_predictions = 0

    for inputs, attention_masks, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs, attention_masks)

        loss = loss_function(outputs.view(-1, num_labels), labels.view(-1))
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        preds = torch.argmax(outputs, dim=-1)
        correct_predictions += (preds == labels).sum().item()
        total_predictions += labels.numel()

    train_loss = running_loss / len(train_loader)
    train_acc = correct_predictions / total_predictions

    # Enregistrer les valeurs dans TensorBoard
    writer.add_scalar("Loss/Train", train_loss, epoch)
    writer.add_scalar("Accuracy/Train", train_acc, epoch)

    print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")

writer.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs

!kill 23696

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir=runs

"""Observation vanishing/ exploding"""

!pip install datasets transformers torch torchcrf tensorboard

import datasets
import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter

# Charger le dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Charger le tokenizer BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de prétraitement des données
def preprocess_data(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le prétraitement
dataset = dataset.map(preprocess_data, batched=True)

# Convertir en format PyTorch
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

def collate_fn(batch):
    """
    Fonction pour traiter les batchs et éviter les warnings.
    """
    return {
        "input_ids": torch.stack([item["input_ids"].clone().detach() for item in batch]),
        "attention_mask": torch.stack([item["attention_mask"].clone().detach() for item in batch]),
        "labels": torch.stack([item["labels"].clone().detach() for item in batch])
    }

batch_size = 16

train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

def train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter("runs/BiLSTM_CRF_gradients")

    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()

            #  Log des gradients et des poids pour analyser le vanishing/exploding gradient
            for name, param in model.named_parameters():
                if param.grad is not None:
                    writer.add_histogram(f"Gradients/{name}", param.grad, epoch)
                writer.add_histogram(f"Weights/{name}", param, epoch)

            optimizer.step()
            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)

        # Planification du Learning Rate
        scheduler.step(avg_train_loss)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_train_loss < best_val_loss:
            best_val_loss = avg_train_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

vocab_size = 30522  # Pour un tokenizer BERT
tagset_size = 9  # Nombre de classes NER

# Instancier le modèle
model = BiLSTM_CRF(vocab_size, tagset_size)

# Lancer l'entraînement
train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005)

import os
log_dir = "runs/BiLSTM_CRF_gradients"
print("Fichiers dans runs/BiLSTM_CRF_gradients:", os.listdir(log_dir) if os.path.exists(log_dir) else "Pas de logs trouvés")

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs/BiLSTM_CRF_gradients

!pip install torch torchvision torchaudio transformers datasets torchcrf tensorboard

!pip install git+https://github.com/kmkurn/pytorch-crf.git

import datasets
import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer
from torch.utils.tensorboard import SummaryWriter

# Charger le dataset CoNLL-2003
dataset = datasets.load_dataset("conll2003")

# Charger le tokenizer BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# Fonction de prétraitement des données
def preprocess_data(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"],
        is_split_into_words=True,
        truncation=True,
        padding="max_length",
        max_length=128
    )

    labels = []
    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples["tokens"]))):
        word_labels = examples["ner_tags"][i]
        label_ids = []

        previous_word_idx = None
        for word_idx in word_ids:
            if word_idx is None:
                label_ids.append(-100)  # Ignorer padding tokens
            elif word_idx != previous_word_idx:
                label_ids.append(word_labels[word_idx] if word_idx < len(word_labels) else -100)
            else:
                label_ids.append(-100)  # Ignorer les sous-tokens
            previous_word_idx = word_idx

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs

# Appliquer le prétraitement
dataset = dataset.map(preprocess_data, batched=True)

# Convertir en format PyTorch
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])

def collate_fn(batch):
    """
    Fonction pour traiter les batchs et éviter les warnings.
    """
    return {
        "input_ids": torch.stack([item["input_ids"].clone().detach() for item in batch]),
        "attention_mask": torch.stack([item["attention_mask"].clone().detach() for item in batch]),
        "labels": torch.stack([item["labels"].clone().detach() for item in batch])
    }

batch_size = 16

train_dataloader = DataLoader(dataset["train"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)
val_dataloader = DataLoader(dataset["validation"], batch_size=batch_size, shuffle=False, collate_fn=collate_fn)

import torch
import torch.nn as nn
import torch.optim as optim
from torchcrf import CRF
from torch.utils.tensorboard import SummaryWriter
from torch.utils.data import DataLoader

# Définition du modèle BiLSTM-CRF
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, dropout=0.5):
        super(BiLSTM_CRF, self).__init__()

        # Embedding Layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)

        # BiLSTM Layer
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2,
                            bidirectional=True, batch_first=True, dropout=dropout)

        # Fully Connected Layer
        self.fc = nn.Linear(hidden_dim, tagset_size)

        # CRF Layer
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, input_ids, attention_mask, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        logits = self.fc(lstm_out)

        if labels is not None:
            labels = labels.clone()
            labels[labels == -100] = 0  # Remplace -100 par une classe neutre

            mask = attention_mask.bool()
            loss = -self.crf(logits, labels, mask=mask, reduction='mean')
            return loss
        else:
            predictions = self.crf.decode(logits, mask=attention_mask.bool())
            return predictions

# Fonction d'entraînement avec logs TensorBoard
def train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005):
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)
    writer = SummaryWriter("runs/BiLSTM_CRF_retrain")
    best_val_loss = float("inf")
    early_stop_patience = 3
    patience_counter = 0

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        correct_predictions = 0
        total_predictions = 0

        for batch in train_dataloader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            optimizer.zero_grad()
            loss = model(input_ids, attention_mask, labels)
            loss.backward()

            optimizer.step()
            total_loss += loss.item()

            # Calcul de l'accuracy
            preds = model(input_ids, attention_mask)
            for i in range(len(preds)):
                correct_predictions += sum([1 for p, l in zip(preds[i], labels[i].tolist()) if l != -100 and p == l])
                total_predictions += sum([1 for l in labels[i].tolist() if l != -100])

        avg_train_loss = total_loss / len(train_dataloader)
        train_accuracy = correct_predictions / total_predictions

        # Enregistrer les métriques dans TensorBoard
        writer.add_scalar("Loss/Train", avg_train_loss, epoch)
        writer.add_scalar("Accuracy/Train", train_accuracy, epoch)

        # Phase de validation
        model.eval()
        total_val_loss = 0
        correct_val_predictions = 0
        total_val_predictions = 0

        with torch.no_grad():
            for batch in val_dataloader:
                input_ids = batch['input_ids']
                attention_mask = batch['attention_mask']
                labels = batch['labels']

                loss = model(input_ids, attention_mask, labels)
                total_val_loss += loss.item()

                preds = model(input_ids, attention_mask)
                for i in range(len(preds)):
                    correct_val_predictions += sum([1 for p, l in zip(preds[i], labels[i].tolist()) if l != -100 and p == l])
                    total_val_predictions += sum([1 for l in labels[i].tolist() if l != -100])

        avg_val_loss = total_val_loss / len(val_dataloader)
        val_accuracy = correct_val_predictions / total_val_predictions

        writer.add_scalar("Loss/Validation", avg_val_loss, epoch)
        writer.add_scalar("Accuracy/Validation", val_accuracy, epoch)
        writer.add_scalar("Learning Rate", optimizer.param_groups[0]["lr"], epoch)

        # Ajustement du learning rate avec le scheduler
        scheduler.step(avg_val_loss)

        print(f"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f} - Train Acc: {train_accuracy:.4f} - Val Loss: {avg_val_loss:.4f} - Val Acc: {val_accuracy:.4f} - LR: {optimizer.param_groups[0]['lr']:.6f}")

        # Early Stopping
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
        else:
            patience_counter += 1
            if patience_counter >= early_stop_patience:
                print("Early stopping triggered.")
                break

    writer.close()
    print("Training complete.")

# Réinitialiser et entraîner à nouveau
vocab_size = 30522  # Tokenizer BERT
tagset_size = 9  # Classes NER

model = BiLSTM_CRF(vocab_size, tagset_size)

train_model(model, train_dataloader, val_dataloader, epochs=20, learning_rate=0.0005)

# Commented out IPython magic to ensure Python compatibility.
# Charger l'extension TensorBoard
# %load_ext tensorboard

# Lancer TensorBoard
# %tensorboard --logdir=runs/BiLSTM_CRF_retrain

import os

log_dir = "runs/BiLSTM_CRF_retrain"
print("Fichiers dans runs/BiLSTM_CRF_retrain:", os.listdir(log_dir))